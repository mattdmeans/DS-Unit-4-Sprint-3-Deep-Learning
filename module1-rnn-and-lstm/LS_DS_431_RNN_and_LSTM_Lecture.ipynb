{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ldr0HZ193GKb"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 1*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "# Neural Networks for Sequences (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad Sequences (samples x time)\n",
      "x_train shape:  (25000, 80)\n",
      "x_test shape:  (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad Sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     1,   778,   128,    74,    12,   630,   163,    15,\n",
       "           4,  1766,  7982,  1051,     2,    32,    85,   156,    45,\n",
       "          40,   148,   139,   121,   664,   665,    10,    10,  1361,\n",
       "         173,     4,   749,     2,    16,  3804,     8,     4,   226,\n",
       "          65,    12,    43,   127,    24, 15344,    10,    10],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 214s 274ms/step - loss: 0.4320 - accuracy: 0.7972 - val_loss: 0.3653 - val_accuracy: 0.8383\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 237s 303ms/step - loss: 0.2482 - accuracy: 0.9019 - val_loss: 0.3758 - val_accuracy: 0.8363\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 243s 311ms/step - loss: 0.1586 - accuracy: 0.9406 - val_loss: 0.4349 - val_accuracy: 0.8299\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 230s 295ms/step - loss: 0.1029 - accuracy: 0.9622 - val_loss: 0.5576 - val_accuracy: 0.8200\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 231s 295ms/step - loss: 0.0712 - accuracy: 0.9750 - val_loss: 0.6608 - val_accuracy: 0.8209\n"
     ]
    }
   ],
   "source": [
    "unicorns = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size, \n",
    "          epochs=5, \n",
    "          validation_data=(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(unicorns.history['loss'])\n",
    "plt.plot(unicorns.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "# LSTM Text generation with Keras (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are some recent headlines from schools around the country: In Indiana, officials played a segment of a 911 call of a teacher in a panic during the Columbine High School shooting to students. In Ohio, officers fired blank shots during an active-shooter drill. In South Carolina, an officer dressed in black posed as an intruder on an unannounced drill. In Michigan, a school is spending $48 million on a renovation that includes curved hallways and hiding niches, in hopes of protecting students from a mass shooting. In Florida, a police officer arrested two 6-year-old students for misdemeanor battery. In Colorado, teachers received buckets and kitty litter for students to use as toilets in case of a prolonged school lockdown.\\n\\nMass shootings, meaning incidents with at least two deaths, in schools are horrifying. But it is highly unlikely that a child would ever witness one. Research indicates that some security measures brought in to make schools safer — like realistic shooter trainings — may be causing children more harm than good.\\n\\nIt is 10 times more likely that a student will die on the way to school.\\n\\nOur chances of dying in a fire are also much greater — 1 in 1,500. But we don’t overreact.\\n\\nMore children have died from lightning strikes than from mass shootings in schools in the past 20 years. Still, we don’t obsess about them.\\n\\nExactly how common are school shootings?\\n\\nIn the two decades since Columbine, there have been 10 mass shootings in schools according to a recent analysis by James Alan Fox, a professor of criminology at Northeastern University who has been studying school violence for several decades. In total, 81 people have been killed, 64 of them students. That’s an average of four deaths per year, three of them students.\\n\\nEven one death is too many. But for perspective, 729 children committed suicide with a firearm in 2017, and 863 were victims of homicides by guns that year.\\n\\nSchool-age children killed by guns 729 suicides in 2017 863 homicides Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University. School-age children killed by guns 729 suicides in 2017 863 homicides Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University. School-age children killed by guns 729 suicides in 2017 863 homicides in 2017 Average killed in school mass shootings each year since 1999 Sources: Centers for Disease Control and Prevention; average students killed since 1999 by James Alan Fox, Northeastern University.\\n\\nNearly every public school in the country now conducts lockdown drills, and even the youngest students participate (last year, one school adapted a lullaby to prepare kindergartners). But very few studies have looked into the efficiency of these drills. One of them concluded that the practice can be helpful to teach students basic safety procedures. But to the author of the study, Jaclyn Schildkraut, an associate professor at the State University of New York at Oswego, there is no point in dramatizing the drills. “All that causes is fear,” she said.\\n\\nRestaurants have 10 times as many homicides as schools. Why do we want to arm teachers and not wait staffs?\\n\\n“There’s a misunderstanding in where the dangers are,” said Dewey G. Cornell, a psychologist and professor at the University of Virginia. “Kids are at far greater danger going to and from school, than they are in the classroom,” he said. “School counseling, academic support, that’s gonna do far more to keep our communities safe.”\\n\\nUnlike the United States, the other wealthy countries in the Group of Seven don’t do lockdown drills and rarely have school shootings. What is the United States doing that is so different from them?\\n\\nGun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017. Gun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017. Gun deaths per 100,000 12 United States 10 8 6 4 Other G7 countries 2 0 25 50 75 100 Guns per 100 people Sources: Institute for Health Metrics and Evaluation, Global Burden of Disease Study 2016; Small Arms Survey, 2017.\\n\\nMany researchers think easy access to guns is an important part of the problem. “Violence in schools is just a small part of the larger problem of gun violence in our society,” Cornell wrote in a statement about prevention of violence in schools and communities.\\n\\nMisguided safety measures, such as dramatized lockdown drills, may give us the impression that we are protecting children, when, in fact, we are handing them a burden that adults are failing to address.\\n\\nRead more:\\n\\n‘What if someone was shooting?’\\n\\nThey grew up practicing lockdown drills. Now they’re steering the conversation on gun violence.\\n\\nSchool shootings are extraordinarily rare. Why is fear of them driving policy?\\n\\nPutting more cops in schools won’t make schools safer, and it will likely inflict a lot of harm'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "# Gather all text \n",
    "# Why? 1. See all possible characters 2. For training / splitting later\n",
    "text = \" \".join(data)\n",
    "\n",
    "# Unique Characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# Lookup Tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  178374\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences: ', len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24,\n",
       " 35,\n",
       " 51,\n",
       " 89,\n",
       " 94,\n",
       " 98,\n",
       " 114,\n",
       " 76,\n",
       " 89,\n",
       " 98,\n",
       " 51,\n",
       " 70,\n",
       " 79,\n",
       " 28,\n",
       " 35,\n",
       " 23,\n",
       " 76,\n",
       " 72,\n",
       " 51,\n",
       " 98,\n",
       " 18,\n",
       " 89,\n",
       " 90,\n",
       " 90,\n",
       " 8,\n",
       " 93,\n",
       " 75,\n",
       " 79,\n",
       " 31,\n",
       " 35,\n",
       " 76,\n",
       " 18,\n",
       " 75,\n",
       " 79,\n",
       " 98,\n",
       " 18,\n",
       " 79,\n",
       " 35,\n",
       " 51,\n",
       " 79]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 121)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5574/5575 [============================>.] - ETA: 0s - loss: 2.5670\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"he Dolphins would have been devastating \"\n",
      "he Dolphins would have been devastating thare aforil exopsien afsayt pible that rime. “The Secoon an, iny pedhise siinite maake, thoike. “pucbutath asticeVowith mos souno teathe-denseplicg prous’s onales af vcely or thes b1ede storians f. zpor colss ines chalting hik ApHory: Pule. Mi\"”\n",
      "\n",
      "“The HEThet nat yobale sid thas lemotee sandes sryabyat ouval inay the intait— comiting pcoacsransedel morle of-deasesus tos a wheb, May to lakiss ,e vi\n",
      "5575/5575 [==============================] - 194s 35ms/step - loss: 2.5670\n",
      "Epoch 2/10\n",
      "5573/5575 [============================>.] - ETA: 0s - loss: 2.2324\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"k up steam during the second half of the\"\n",
      "k up steam during the second half of the Wemp ay in on Tullmiker Trevesing,” the Sextam, btomely an the to ness. Thencur ditan, weissing pextore for surnelieg zore to Bowsher tre hes is checle’s Rome the godlis nowe cplesh Rinta ost comm. Thisurene\" “ThkiLe Frupecton\n",
      "\n",
      "Caufionter atettonfould ble stere was mesing’ palt. I int. Macue and! Hese and hacley. “2t wate sun thing on Putsory Insseating and counten and sile arm sov righing foryer\n",
      "5575/5575 [==============================] - 212s 38ms/step - loss: 2.2324\n",
      "Epoch 3/10\n",
      "5574/5575 [============================>.] - ETA: 0s - loss: 2.1038\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \" turned out — as it often turns out for \"\n",
      " turned out — as it often turns out for a teighter,” “The U.BMory and Trumy hus nof urely efarcone. Is leess wake to expastions. 13 yion, shame, sue udessrand gale saventiotion. They is speor eviigs to thew’s in illose sumition mach nour it in usentees” reabonst of an Fably a byert sumpy. Min ary-onnigallifn\n",
      "\n",
      "Doulan4 scabucant. “to hor croviss dident ame the vooner the ceritions allevictes heiger your gal on rulps reandem will the sping\n",
      "5575/5575 [==============================] - 256s 46ms/step - loss: 2.1038\n",
      "Epoch 4/10\n",
      "5575/5575 [==============================] - ETA: 0s - loss: 2.0108\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \" with a congressional subpoena and inten\"\n",
      " with a congressional subpoena and intenemsions on Pasiee a Wence Ebsoonen ispertens worder swite Pestroman Certarion’s whecksthtent und tho plose Gonst ant Jonging … Stherres a. Hous-hast oveaby\n",
      "\n",
      "Texteresparment to 66 as me wover staticu, an itrect ance mode in fitits” wo geet traccred wicled beingtanizicied furnous le fut a vigens to arvire aid thes trueno, the Surdocary (fol a DeCtinaG costting wite in a with mic stuling ow the respo\n",
      "5575/5575 [==============================] - 307s 55ms/step - loss: 2.0108\n",
      "Epoch 5/10\n",
      "5575/5575 [==============================] - ETA: 0s - loss: 1.9370\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"on 230 of the Communications Decency Act\"\n",
      "on 230 of the Communications Decency Actcesscy of hid not of fould inandoo cittrouss butuiled of the Whelperal and hereszery, Uncause”, “S-stroath one thethirg is playia coipentire\n",
      "\n",
      "Sheth’l “3-tame CabliC Solish U.s.in Zeanseerso for suvery deardy masidinbdey, thep is than’d persible informiticaricus it blose sursing corlincrion wir you insormandd exposicaty hat We in the deppetion scint she be the care. “I becon apparts.”\n",
      "\n",
      "Haslices.\n",
      "\n",
      "T\n",
      "5575/5575 [==============================] - 393s 71ms/step - loss: 1.9370\n",
      "Epoch 6/10\n",
      "5573/5575 [============================>.] - ETA: 0s - loss: 1.8752\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"arting supplements, so they can check in\"\n",
      "arting supplements, so they can check in the epcoplesion thyir the Fertmed Prexideits toguchie Hhitters as Meblocker Crobpt wayner SAS MCEPO ANTTAmo TTIA HELAMUTONITRUSSIE COA CAMIN OR UIS OR THE WAIES SOY IAY ANT WITI BE ANF YIWU) \n",
      "IT INE SFFWAY IUNS OT RMCESTICISPINNERPOA Od TVINC SILIIGW, AN THE THICT (TSVICCE LAYALE APCE RARICN FILGTOUnIPTTERICESNION RAADS IN NY PESt ADVADISTINT AO YYITBITTABL, SIUR INE WATITIMALLAANL IS TORAS THE L\n",
      "5575/5575 [==============================] - 236s 42ms/step - loss: 1.8753\n",
      "Epoch 7/10\n",
      "5573/5575 [============================>.] - ETA: 0s - loss: 1.8227\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"d the story about America’s open arms fo\"\n",
      "d the story about America’s open arms foors that weuntslub. In paresedsion dased appilition Kanks, be humpeadlicu somition scing comule the regination prrorid.”\n",
      "\n",
      "dranged to “IN, gon strects complaters dischine and treated, brounderes as promback ferm Issowh, bockratiat, compents, bocking amowh fill of the U.S. firvulting flact conted livinits in the ssaity, sight, who whh becontet to than wlead.\n",
      "\n",
      "AD\n",
      "\n",
      "Kebone parentes lifele, the NFres, c\n",
      "5575/5575 [==============================] - 178s 32ms/step - loss: 1.8227\n",
      "Epoch 8/10\n",
      "5574/5575 [============================>.] - ETA: 0s - loss: 1.7772\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"omising a “complete and thorough” probe \"\n",
      "omising a “complete and thorough” probe tems dessing and some heved and was about the statem. entords who over incloded, Derielics. An1 whongly for himen Grudsin grenip the get, Fides is a sendidents, an anter ale curties, shist om his a with peropling in its bloagh compori, one on gressly and publity respoons detiverster engime as “hay that. They, while to thet repercementive the raid Streatilia for a.\n",
      "\n",
      "A phopppay. Washe untws Collent \n",
      "5575/5575 [==============================] - 172s 31ms/step - loss: 1.7772\n",
      "Epoch 9/10\n",
      "5573/5575 [============================>.] - ETA: 0s - loss: 1.7385\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \"e was complete, the helicopter soared up\"\n",
      "e was complete, the helicopter soared up and have geabole should to enthin”\n",
      "\n",
      "Werie and Mock Accoldots comm. 25 trayfone” adains came inteltedn. The hare tugan, D.S). They now with becreased what the about postations the leadly satulf-years it Vinglstent wheth any puricy liginatirits say the Distry, with lify nuarly playsed,” \n",
      "\n",
      "“Lea worch by olliers. Reak milling the rogramed buach, and whinglused thif haverwerly, you isligia workenoliti\n",
      "5575/5575 [==============================] - 173s 31ms/step - loss: 1.7385\n",
      "Epoch 10/10\n",
      "5574/5575 [============================>.] - ETA: 0s - loss: 1.7044\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"During his clumsy and tone-deaf news con\"\n",
      "During his clumsy and tone-deaf news contritied growes to impust mentary of contuntin” 153: The Infirent any theid nerve impences moticauter’s and “saysivers before anown Petse Galest masker dreasure joclity and publiciats opporter Cloim defendandsurul, pachment or a presement reag-arey, story woll orioual out fare, and driens agon the boad its expenized a wighmern ancopital of Wiscosa, stulthing punitual langry officional deat of I Dad\n",
      "5575/5575 [==============================] - 170s 31ms/step - loss: 1.7044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd47492e860>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use a Keras LSTM to generate text on today's assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "    * Sequence Problems:\n",
    "        - Time Series (like Stock Prices, Weather, etc.)\n",
    "        - Text Classification\n",
    "        - Text Generation\n",
    "        - And many more! :D\n",
    "    * LSTMs are generally preferred over RNNs for most problems\n",
    "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
    "    * Keras has LSTMs/RNN layer types implemented nicely\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
    "    * Shape of input data is very important\n",
    "    * Can take a while to train\n",
    "    * You can use it to write movie scripts. :P "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S1_NLP",
   "language": "python",
   "name": "u4-s1_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
